---
author: "Julia"
date due: "10/30/2020"
output: html_document
---

# Homework 7: Cross-Validation and Bayes {.tabset}

#### Julia McCartney  
#### Due 10/30/2020

This homework's file can also be found on [Github](https://github.com/julia-mccartney/McCartney_Biol_607/blob/master/homework/07_McCartney_Julia_2020.Rmd)

  
Libraries Used
```{r Libraries, message=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(purrr)
```


For today, we’ll consider data from [Brutsaert et al. 2002](https://jeb.biologists.org/content/jexbio/205/2/233.full.pdf) looking at how progestrone levels influence respiration at altitude. The data can be downloaded [here](https://whitlockschluter.zoology.ubc.ca/wp-content/data/chapter17/chap17q07ProgesteroneExercise.csv) with progestrone levels and ventilation as a metric of breathing.

```{r data, message=FALSE}
# load the data
resp <- readr::read_csv("../data/chap17q07ProgesteroneExercise.csv")

```



## 1. Create models with different polys
Let’s first look at the data. Plot it, along with a polynomial fit (remember, formula = y ~ poly(x,2) for a quadratic). Then, compare the r2 value of a linear versus fith order fit. What do you see?


```{r fit and graph}
# create polynomial fits to the data

resp_lm <- lm(ventilation ~ poly(progesterone,1), data = resp)
resp_sq <- lm(ventilation ~ poly(progesterone, 2), data = resp)
resp_cub <- lm(ventilation ~ poly(progesterone, 3), data = resp)
resp_4 <- lm(ventilation ~ poly(progesterone, 4), data = resp)
resp_5 <- lm(ventilation ~ poly(progesterone, 5), data = resp)
resp_int <- lm(ventilation ~ 1, data = resp)


# graph the data
ggplot(data = resp, 
       aes(x = progesterone, y = ventilation)) +
  geom_point() +
  theme_minimal() +
  labs(x = "Progesterone (pg/ml)",
       y = "Ventilation rate (ml/min",
       caption = "Data from Brutsaert et al. 2001",
       color = "Polynomial") +
  stat_smooth(method = "lm", fill = NA, formula = y ~ x, aes(color = "1st order"), show.legend = TRUE ) +
  stat_smooth(method = "lm", fill = NA, formula = y ~ poly(x,2), aes(color = "2nd order"), show.legend = TRUE) +
  stat_smooth(method = "lm", fill = NA, formula = y ~ poly(x,3), aes(color = "3rd order"), show.legend = TRUE) +
  stat_smooth(method = "lm", fill = NA, formula = y ~ poly(x,4), aes(color = "4th order"), show.legend = TRUE) +
  stat_smooth(method = "lm", fill = NA, formula = y ~ poly(x,5), aes(color = "5th order"), show.legend = TRUE) +
  scale_color_brewer(palette = "Paired") +
  ylim(min = 50, max = 120)

```

```{r compare fits}
# Compare the r2 of linear and 5th order fit

linear <- summary(resp_lm)
fifth_order <- summary(resp_5)
formattable::formattable(data.frame(linear$r.squared, fifth_order$r.squared))
```

From the r2 values, it appears that the linear model is a better fit (though the difference in r2 is not extreme). Looking that the graph above this intuitively makes sense, as the data appear linear and tightly clustered at the low end of progesterone concentration, and become more scattered as it increases. 

## 2. Fit each model with 5-fold CV

Does that result hold up, or is it due to overfitting? Let’s evaluate by comparing 5-fold CV scores using RMSE. Let’s do this efficiently, though!

#### A. Get things ready! 
Make a 5-fold cross validation tibble using rsample::vfold_cv() and then combine each possible fold with the polynomials 1:5 using tidyr::crossing()

```{r 2a}
# set up for kfold sampling and cv

set.seed(27102020)
resp_poly <- rsample::vfold_cv(resp, v = 5) %>% 
  tidyr::crossing(., polyn = 1:5 )

```


#### B. Now you have splits and a column of coefficients. 
Use purr::map2() to make a list column of fit models, where you use the splits and data and the polynomials for you poly() call in the model.

```{r 2b}
# create polynomial models
resp_cv <- resp_poly %>% 
  mutate(mods = map2(splits, polyn, 
                     ~lm(ventilation ~ poly(progesterone, .y), data = rsample::analysis(.x))))

```


#### C. Great! Now, calculate the rmse for each fold/polynomial combination as we did in lab.

```{r 2c}
# calculate RMSE
resp_cv_rmse <- resp_cv %>% 
  mutate(rmse = map2_dbl(splits, mods, 
                         ~modelr::rmse(model = .y, 
                               data = rsample::assessment(.x))))

```


#### D. Implications
Ok, given that the 5-fold score is the average RMSE across all folds for a given polynomial, show in both a table and figure the relationship between polynomial and out-of-sample RMSE. What does this tell you?

```{r 2d 1}
# make table of rmse and polynomials

polyn_rmse <- data.frame(resp_cv_rmse$id, resp_cv_rmse$polyn, resp_cv_rmse$rmse) %>% 
  arrange(by_group = resp_cv_rmse.polyn)

formattable::formattable(polyn_rmse)
```

This seems to vary greatly by fold and doesn't tell us too much like this - lets take a look at the average by polynomial.


```{r 2d 2, message=FALSE}
# look at the average rmse by polynomial

polyn_rmse %>% 
  group_by(resp_cv_rmse.polyn) %>% 
  summarize(av_rmse = mean(resp_cv_rmse.rmse), 
            sd = sd(resp_cv_rmse.rmse)) %>% 
  formattable::formattable()
```

From this table, we can see that the 1st order polynomial (the linear equation) appears to fit the data the best according to RMSE (and had the least variation of this value across folds).



```{r 2d 3}
# graph the data

ggplot(data = polyn_rmse,
       aes(x = resp_cv_rmse.polyn, y = resp_cv_rmse.rmse, color = resp_cv_rmse.id)) +
  geom_point() +
  labs(x = "Polynomial Order",
       y = "RMSE",
       color = "k fold")


```

The differences between folds are clear here, but so is the somewhat linear increase of RMSE as the polynomial order increases (indicating a worse fit).


## 3. Compare models and see how they differ from AIC
That was all well and good, but, how to these results compare to doing this analysis with AIC using the `{AICcmodavg}` package? Note, you can use dplyr and purrr to not have to fit each model manually.

Well I already fit them all manually so...

```{r 3}

mod_list <- list(resp_int, resp_lm, resp_sq, resp_cub, resp_4, resp_5)
mod_names <- c("Intercept Model", "1st order", "2nd order", "3rd order", "4th order", "5th order")
formattable::formattable(AICcmodavg::aictab(cand.set = mod_list, modnames = mod_names))

```

The AIC values indicate what the r2 and RMSE values did - the linear model is the best fit for this data

## 5. Grid sample with Bayes

Last week, we did grid sampling with Likelihood. This week, let’s do it with Bayes!

p(H|D)=p(D|H)p(H)p(D)

#### A. Let’s start with the Palmer Penguins data. 
Let’s look at just the Gentoo. Why don’t you plot the distribution of the average flipper length of females. We’ll use this data for the exercise. Remember to remove NAs - it will make the rest of the exercise easier. 1 EC for each thing you do to snaz the plot up.

```{r 5a data loading}
# load in data
library(palmerpenguins)

# filter data for just Gentoo
Gentoo <- penguins %>% 
  filter(species == "Gentoo") %>% 
  na.omit(na.rm = TRUE)

# plot data
Gentoo %>% 
  filter(sex == "female")  %>% 
  ggplot(aes(x = flipper_length_mm)) +
  geom_density() +
  theme_minimal() +
  labs(x = "Flipper Length (mm)",
       y = "Density",
       title = "Distribution of Female\nGentoo Flipper Length")
```


#### B. OK, this is pretty normal, with a mean of 212.71 and sd of 3.9. 
Make a grid to search a number of values around that mean and SD, just as you did for likelihood. Let’s say 100 values of each parameter.

```{r 5b}
#Make a grid for searching
gentoo_grid <- crossing(m = seq(209,222, length.out = 100), 
                  s = seq(2,10, length.out = 100)) 

```


#### C. Write a function that will give you the numerator for any combination of m and s!

This is just the same as writing a function for likelihood, but including an additional multiplier of p(H), when putting in the likelihood. Let’s assume a prior for m of dnorm(210, 50) and for s of dunif(1,10) - so, pretty weak!

So, we want p(m, s|flipper length)*p(m)*p(s).

BUT - small problem. These numbers get so vanishingly small, we can no longer do calculations with them. So, for any probability density you use, add log=TRUE and take a sum instead of products or multiplication, as

log(p(D|H)p(H))=log(p(D|H))+log(p(H))

```{r 5c}
# Write a function to determine the likelihood x prior values

Bayes_lm <- function(m, s){
  likelihood <- sum(dnorm(x = Gentoo$flipper_length_mm, mean = m, sd = s, log = TRUE))
    pm <- sum(dnorm(m, 210,50, log = TRUE))
    ps <- sum(dunif(x = s,min = 1,max = 10, log = TRUE))
    out <- likelihood+pm+ps
    return(out)
}

Bayes_lm(m = 212.71, s = 3.9)
```


#### D. Great! Now use this function with your sample grid to get the numerator of the posterior, and then standardize with the p(D) - the sum of all numerators - to get a full posterior. 

Note, as we’re working in logs, we just subtract log(p(D)) What is the modal estimate of each parameter? How do they compare to the standard frequentist estimate?

Note: log(p(d)) = log(sum(exp(p(D|H)p(H))))

```{r 5d}
# calculate numerator values
gentoo_grid_sample <- gentoo_grid %>% 
  rowwise() %>% 
  mutate(numerator = Bayes_lm(m = m, s = s))
  
# get posterior
# log(p(D|H)p(H))=log(p(D|H))+log(p(H))

# get probability of data p(D)
pd <- gentoo_grid_sample %>% 
  mutate(pdh_ph = exp(numerator))

pd_sum <-  log(sum(pd$pdh_ph))

gento_grid_post <- 
  gentoo_grid_sample %>% 
  mutate(posterior = exp(numerator - pd_sum))

max_vals <- gento_grid_post %>% 
  filter(posterior == max(gento_grid_post$posterior)) 

# Bayesian Estimates
formattable::formattable(max_vals)

```

Now to look at the frequenitst estimates...

```{r 5d 2}
# Get the frequentest estimates of the same data

# write a function
norm_loglik <- function(m, s, data){
  dnorm(data, mean = m, sd = s, log = TRUE) %>% sum()
}

# grid sample
gentoo_freq_sample <- gentoo_grid %>% 
  rowwise() %>% 
  mutate(log_lik = norm_loglik(m , s, data = Gentoo$flipper_length_mm
                               ))
  
# get max estimate
gentoo_freq_sample %>% filter(log_lik == max(gentoo_freq_sample$log_lik)) %>% as.data.frame() %>% formattable::formattable()


```


These data actually match pretty closely! The prior is fairly weak, so that probably explains the lack of difference between the two methods.

#### E.C. E. Show me ’dat surface! Make it sing!

```{r 5 EC, warning=FALSE}

# graph priors
gentoo_surface <- ggplot(data = gento_grid_post,
                         aes(x = m, y = s, z = posterior, color = posterior)) +
  geom_contour_filled(bins = 50) +
  labs(x = "Mean",
       y = "Standard Deviation") +
  guides(fill = "none") 
gentoo_surface

# let's adjust
gentoo_surface +
  xlim(215.5,219) + # get in a bit closer
  ylim(5.5,8)

```
#### E.C. x2 F Compare our weak prior to one with a strong prior.
Note, as you progress in this, instead of doing the product of p(D|H)p(H), you might want to do log(p(D|H)) + log(p(H)) as it simplifies calculations. The nice thing is then you just subtract log(p(D)) to get log(p(H|D)) - which you can then safely exponentiate!

```{r 5 ec pt 2}
# grid sample with bayes with strong prior

# adjust priors
Bayes_lm_strong <- function(m, s){
  likelihood <- sum(dnorm(x = Gentoo$flipper_length_mm, mean = m, sd = s, log = TRUE))
    pm <- sum(dnorm(m, 300,15, log = TRUE)) # I think these count as strong priors?
    ps <- sum(dunif(x = s,min = 7, max = 15, log = TRUE))
    out <- likelihood+pm+ps
    return(out)
}

# calculate numerator values
gentoo_grid_sample_strong <- gentoo_grid %>% 
  rowwise() %>% 
  mutate(numerator = Bayes_lm_strong(m = m, s = s))
  
# get posterior
# log(p(D|H)p(H))=log(p(D|H))+log(p(H))

# get probability of data p(D)
pd_s <- gentoo_grid_sample_strong %>% 
  mutate(pdh_ph = exp(numerator))

pd_sum_s <-  log(sum(pd_s$pdh_ph))

gento_grid_post_s <- 
  gentoo_grid_sample_strong %>% 
  mutate(posterior = exp(numerator - pd_sum))

max_vals_s <- gento_grid_post_s %>% 
  filter(posterior == max(gento_grid_post_s$posterior)) 

# Bayesian Estimates
formattable::formattable(max_vals_s)


# compared to the original 
formattable::formattable(max_vals)
```

Well, even with what I thought were strong priors, I couldn't shift the data all that much! I was definitely easier to shift the standard deviation, but even then I only moved it by ~0.5 units. The data set is large enough to overwhelm the priors!

### 6. Final Project Thinking
We’re at the half-way point in the course, and after the mid-term, it’s time to start thinking about your final project. So…. I want to know a bit about what you’re thinking of!

#### A. What is the dataset you are thinking of working with? Tell me a bit about what’s in it, and where it comes from.

The dataset I'm looking to work with is 16S and ITS sequencing data from skin swabs taken during a survey of Eastern Newts from across the Eastern US, covering over 20 sampling sites. This data comes from fieldwork done by a post-doc in my lab (COVID19 + long exposure experiments + LOTS of samples to process = very little of my own data currently available ):

#### B. What question do you want to ask of that data set?

I am looking to understand if there are any patterns of taxonomic co-occurance between fungi (ITS sequencing) and bacteria (16S sequencing) on Eastern Newts across the sampled range. There is also similar data available from exposure experiments run on individuals collected from some of the sample sites, so perhaps connections can be drawn between susceptibility and co-occurance, but that may be outside of the scope of this work.


