---
author: "Julia"
date: "10/18/2020"
output: html_document
---
# Homework 6 {.tabset}

#### Julia McCartney  
#### Due 10/23/2020

This homework can also be found on [github](https://github.com/julia-mccartney/McCartney_Biol_607/blob/master/homework/06_McCartney_Julia_2020.Rmd)

Libraries
```{r libraries, message = FALSE}
library(readr)
library(tidyr)
library(ggplot2)
library(dplyr)
library(profileModel)
library(formattable)
library(MASS)
```



#### 0. Faded examples

I feel pretty good about them. No issues here!

## 1. Inference

**Would you say you naturally gravitate towards deductive or inductive inference? Why?**

I feel like I sit somewhere in the middle depending on what the situation is. If I am thinking about a system that is well studied and I have a good grasp on it and the factors that influence it, I think I lean more towards deductive. This is because I find it easier to come up with a theory to explain something. When I'm thinking of a system that is not well studied, I feel uncomfortable coming up with theories right off the bat and lean more towards inductive reasoning. All that being said, I can see where this can stifle novel thoughts about a well studied system (maybe creating some kind of confirmation bias?). I also feel like my understanding of how to design an experiment is heavily reliant on deductive reasoning, so in practice I likely fall into this category much more often. 


## 2 Research planning
We talked about strictly interpreted Popperian Flasification versus Lakatos’s view of a research program this week.

### 2a Do you more strongly identify with one of these paradigms? 
Why? +1 EC for direct quotes (if you want to do some additional reading)

I would say the way I think about conducting experiments falls more along the lines of Popper's thinking (this is more the result of how I was taught more than anything else). How I think about large scale problems or research topics in science, however, probably falls more along the line of Lakatos's research program approach. I find the program idea much easier to think about in a broader context, but it breaks down for me at the small scale.


### 2b. How does your own research program fit into one of these paradigms?

One of the questions I'm interested in is understanding how bacterial communities and change assemble during/after a disturbance like disease - essentially do we see a stochastic or deterministic response? Because it's a bit more exploratory at this stage, I think (contrary to what I said before), this best fits in with Lakotos's view of a research program. We have a central theory that microbiome communities change as a result of disturbance during disease. Why that happens though is still unclear - does the microbiome independently react to pathogens?, does the host influence the community as an immune response? does the community change as a byproduct of an immune response? is there no shift reaction at all? Then there are the questions about *how* the community is changing...

I'm also interested in looking at the functional capacity of the microbiome (not just the taxonomic makeup), so that adds a whole other layer to the questions (Do we see a stochastic/deterministic response both at the taxonomic and functional levels? Just one? neither?).

Additionally, all of this can vary by pathogen and host organism (but for now, I'm focused on Eastern Newts and *Batrachochytrium salamandrivorans*). 

When I was trying to visualize this, Lakotos's system (at least as I understand it) was naturally the easiest way for me to conceptualize all of the possibilities of what may be going on.


## 3. Grid Sampling

Based on Friday’s lab, load up the pufferfish data and use grid sampling to find the MLE of the slope, intercept and residual SD of this model. Feel free to eyeball results from an lm() fit to get reasonable values. Try not to do this for a grid of more than ~100K points (more if you want!). It’s ok to be coarse. Compare to lm.

```{r load, message = FALSE}
# load data

puffer <- read_csv("../data/16q11PufferfishMimicry Caley & Schluter 2003.csv")
```

```{r 3 graph, message= FALSE}
#Graph the data to get a better sense of it
ggplot(puffer,
       aes(x = resemblance, 
           y = predators)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Resemblance to Puffer Fish",
       y = "Number of Predator Visits")
```

Looking at this fit, it seems like we have a positive slope (maybe around 2-3) and an intercept around 5. Residual standard deviation looks to be around 3-5. We will use these estimates to set ranges for calculating the maximum likelihood estimate for each parameter.




We can start by building a function to get the MLEs:
```{r 3 mle function}
# Making this a function in the vague hope that maybe it will be useful one day

MLE_lm <- function(xvar, yvar, res_sdev, slope, int){
  
  fitY <- xvar * slope + int
  sum(dnorm(fitY, yvar, res_sdev, log = TRUE))
  
}
```

```{r 3 grid sample}
# grid sample
puff_grid <- crossing(int = seq(1,5, length.out = 100), # sorry its so big, the result was better with 100 points each
                  slope = seq(1,5, length.out = 100),
                  res_sd = seq(1,5, length.out = 100)) %>%
  rowwise(slope, int, res_sd) %>%
  mutate(logLik = MLE_lm(slope = slope, int = int, res_sdev = res_sd, yvar = puffer$predators, xvar = puffer$resemblance),
         deviance = -2*logLik) %>%
  ungroup()

# get our max values
puff_grid %>% filter(logLik == max(logLik)) %>% as.data.frame() %>% formattable()

# Compare to the LM
puff_lm <- lm(predators ~ resemblance, data = puffer)

formattable(puff_lm$coefficients)

```

Looks like we came pretty close with our estimates!


## 4. Surfaces!
Filter the dataset to the MLE of the SD. Plot the surface for the slope and intercept in whatever way you find most compelling. You might want to play around with zooming in to different regions, etc. Have fun!

```{r 4 filter}
# First, the actual number is long and r needs a direct output
max_vals <- puff_grid %>% filter(logLik == max(logLik)) %>% as.data.frame()

#Filter the data
puff_max_res_sd <- puff_grid %>% 
  filter(res_sd == max_vals$res_sd)

```

```{r 4 graph, message = FALSE, warning=FALSE}

# calculate profiles
lik_prof_slope <- puff_max_res_sd %>% 
  group_by(slope) %>% 
  filter(deviance == min(deviance)) %>% 
  ungroup()

lik_prof_int <- puff_max_res_sd %>% 
  group_by(int) %>% 
  filter(deviance == min(deviance)) %>% 
  ungroup()

# start with a basic graph like we did in lab

plot <- ggplot(data = puff_max_res_sd %>%  filter(logLik > max(logLik) - 3),
       aes(x = slope, y = int, z = logLik, color = logLik)) +
  geom_contour_filled(bins = 20) + 
  labs(x = "Slope",
       y = "Intercept") +
  guides(fill = "none") +
  xlim(2.25,3.75) + # get in a bit closer
  ylim(1,4) +
    # and add in the profiles
  geom_line(data = lik_prof_slope %>%  filter(logLik > max(logLik) - 1.92),
            aes(x = slope, y = int), color = "red") +
  geom_line(data = lik_prof_int %>%  filter(logLik > max(logLik) - 1.92),
            aes(x = slope, y = int), color = "orange")

plot

```

Ok lets zoom in on that 

```{r}
# get better resolution
zoom_plot <-  ggplot(data = puff_max_res_sd %>%  filter(logLik > max(logLik) - 3),
       aes(x = slope, y = int, z = logLik, color = logLik)) +
  geom_contour_filled(bins = 20) + 
  labs(x = "Slope",
       y = "Intercept") +
  guides(fill = "none") +
  xlim(2.75,3.5) + # get in a bit closer
  ylim(1.5,3) +
    # and add in the profiles
  geom_line(data = lik_prof_slope %>%  filter(logLik > max(logLik) - 1.92),
            aes(x = slope, y = int), color = "red") +
  geom_line(data = lik_prof_int %>%  filter(logLik > max(logLik) - 1.92),
            aes(x = slope, y = int), color = "orange")

zoom_plot
```

And let's have some fun with it

```{r fun plot, warning=FALSE}
# make a 3d plot
rast_plot <- ggplot(data = puff_max_res_sd %>%  filter(logLik > max(logLik) - 3),
       aes(x = slope, y = int, fill = logLik)) +
  geom_raster() + 
  scale_fill_viridis_b()+
  labs(x = "Slope",
       y = "Intercept")

rayshader::plot_gg(rast_plot +
                     geom_line(data = lik_prof_slope %>%  filter(logLik > max(logLik) - 1.92),
            aes(x = slope, y = int), color = "red") +
  geom_line(data = lik_prof_int %>%  filter(logLik > max(logLik) - 1.92),
            aes(x = slope, y = int), color = "orange"))


```

## 5. GLM!
Now, compare those results to results from glm. Show the profiles and confidence intervals from glm() for the slope and intercept. Also show how you validate assumptions.

```{r 5 build GLM}
# Build the model
puff_mod <- glm(predators ~ resemblance, data = puffer, 
                family = gaussian(link = "identity"))

# test assumptions
puff_fit <- predict(puff_mod)
puff_res <- residuals(puff_mod)

qplot(puff_fit, puff_res)

```

We can see that there does not appear to be any relationship between the residuals and the fit values (assumption maintained).

```{r 5 more assumptions}
qqnorm(puff_res) 
qqline(puff_res)
```

The data does deviate from the line (and looking at the qqnorm plot on its own, the data do not look very linear). However, the deviation isn't so big that we need to be worried (assumption tentatively maintained).

```{r 5 profile}
puff_prof <- profile(puff_mod)
plot(puff_prof)
```

The profiles are straight lines, as they shold be. (Assumption maintained)


Now to look at the CIs

```{r 5 confidence intervals}
plot(profileModel(puff_mod,
                     objective = "ordinaryDeviance",
                     quantile = qchisq(0.95, 1)))
```

```{r 5 CIs, message = FALSE}
formattable(confint(puff_mod))
```

Now to compare to our grid sampling
```{r 5 compare}
# from our GLM
formattable(puff_mod$coefficients)

```

```{r 5 compare 2}
formattable(puff_grid %>% filter(logLik == max(logLik)) %>% as.data.frame())
```

Our grid sampling results are pretty close to the GLM results!


